{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import array\n",
    "from pandas import DataFrame, concat\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of             AF3        F7        F3       FC5        T7        P7        O1  \\\n",
      "0      0.010702  0.236931  0.556326  0.002648  0.515557  0.005053  0.003558   \n",
      "1      0.010687  0.236004  0.557117  0.002649  0.513684  0.005054  0.003559   \n",
      "2      0.010697  0.236416  0.557379  0.002661  0.512514  0.005046  0.003558   \n",
      "3      0.010701  0.237446  0.557556  0.002660  0.514035  0.005043  0.003559   \n",
      "4      0.010692  0.237446  0.556854  0.002653  0.514971  0.005054  0.003556   \n",
      "5      0.010676  0.236004  0.555448  0.002656  0.514503  0.005056  0.003552   \n",
      "6      0.010671  0.235283  0.554833  0.002653  0.514035  0.005048  0.003546   \n",
      "7      0.010691  0.236416  0.554482  0.002640  0.514152  0.005044  0.003541   \n",
      "8      0.010692  0.237241  0.554131  0.002634  0.514387  0.005047  0.003548   \n",
      "9      0.010692  0.237343  0.554219  0.002638  0.514152  0.005043  0.003551   \n",
      "10     0.010692  0.237241  0.553517  0.002640  0.513801  0.005034  0.003542   \n",
      "11     0.010662  0.235590  0.551234  0.002629  0.513100  0.005040  0.003539   \n",
      "12     0.010634  0.233734  0.549214  0.002611  0.512282  0.005048  0.003546   \n",
      "13     0.010634  0.234766  0.549916  0.002608  0.512865  0.005048  0.003548   \n",
      "14     0.010649  0.236312  0.551409  0.002616  0.513452  0.005053  0.003546   \n",
      "15     0.010652  0.235797  0.550620  0.002616  0.512749  0.005053  0.003548   \n",
      "16     0.010629  0.234663  0.549127  0.002608  0.512514  0.005047  0.003554   \n",
      "17     0.010619  0.234354  0.549478  0.002607  0.513452  0.005047  0.003547   \n",
      "18     0.010647  0.235075  0.550620  0.002610  0.513684  0.005054  0.003538   \n",
      "19     0.010659  0.235797  0.550883  0.002608  0.513568  0.005058  0.003543   \n",
      "20     0.010654  0.236209  0.551234  0.002609  0.514387  0.005058  0.003550   \n",
      "21     0.010687  0.237653  0.551849  0.002623  0.513919  0.005056  0.003549   \n",
      "22     0.010709  0.239921  0.552375  0.002635  0.512398  0.005051  0.003546   \n",
      "23     0.010694  0.239096  0.552024  0.002631  0.511930  0.005053  0.003543   \n",
      "24     0.010677  0.236622  0.551322  0.002623  0.513217  0.005051  0.003541   \n",
      "25     0.010664  0.236622  0.550883  0.002617  0.514035  0.005050  0.003543   \n",
      "26     0.010646  0.236519  0.549741  0.002612  0.512398  0.005056  0.003542   \n",
      "27     0.010636  0.235075  0.548776  0.002606  0.510760  0.005051  0.003540   \n",
      "28     0.010634  0.235488  0.550092  0.002608  0.511579  0.005043  0.003542   \n",
      "29     0.010644  0.237343  0.552200  0.002620  0.513100  0.005046  0.003546   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14950  0.010706  0.242396  0.555097  0.002637  0.513452  0.005133  0.003507   \n",
      "14951  0.010691  0.242603  0.554570  0.002643  0.512282  0.005118  0.003505   \n",
      "14952  0.010694  0.242910  0.554833  0.002649  0.511930  0.005103  0.003505   \n",
      "14953  0.010697  0.242808  0.554833  0.002645  0.513217  0.005117  0.003514   \n",
      "14954  0.010701  0.242603  0.554659  0.002649  0.513684  0.005137  0.003518   \n",
      "14955  0.010699  0.242498  0.554922  0.002656  0.512282  0.005137  0.003520   \n",
      "14956  0.010679  0.242086  0.554746  0.002653  0.511463  0.005127  0.003523   \n",
      "14957  0.010659  0.241467  0.553955  0.002649  0.512633  0.005131  0.003524   \n",
      "14958  0.010659  0.240643  0.554044  0.002649  0.512982  0.005148  0.003525   \n",
      "14959  0.010647  0.239509  0.554482  0.002649  0.512514  0.005157  0.003531   \n",
      "14960  0.010621  0.238477  0.553693  0.002643  0.513217  0.005157  0.003536   \n",
      "14961  0.010617  0.238477  0.553166  0.002633  0.513684  0.005151  0.003533   \n",
      "14962  0.010622  0.238787  0.553429  0.002634  0.513217  0.005145  0.003533   \n",
      "14963  0.010611  0.237860  0.552815  0.002635  0.513452  0.005140  0.003532   \n",
      "14964  0.010607  0.236931  0.551936  0.002633  0.513801  0.005141  0.003535   \n",
      "14965  0.010616  0.236829  0.552200  0.002633  0.512398  0.005143  0.003544   \n",
      "14966  0.010601  0.236312  0.552375  0.002630  0.511112  0.005135  0.003543   \n",
      "14967  0.010588  0.236107  0.551760  0.002627  0.511579  0.005131  0.003535   \n",
      "14968  0.010604  0.236416  0.551498  0.002631  0.512749  0.005134  0.003541   \n",
      "14969  0.010597  0.236312  0.552024  0.002627  0.513100  0.005145  0.003547   \n",
      "14970  0.010573  0.235695  0.551936  0.002611  0.512865  0.005150  0.003536   \n",
      "14971  0.010571  0.234973  0.550356  0.002605  0.512514  0.005144  0.003533   \n",
      "14972  0.010569  0.234251  0.549303  0.002604  0.511930  0.005135  0.003537   \n",
      "14973  0.010551  0.233425  0.549654  0.002597  0.511463  0.005127  0.003524   \n",
      "14974  0.010544  0.232808  0.549478  0.002599  0.511463  0.005127  0.003515   \n",
      "14975  0.010546  0.233117  0.548863  0.002599  0.511814  0.005131  0.003519   \n",
      "14976  0.010533  0.233425  0.548776  0.002589  0.511579  0.005134  0.003517   \n",
      "14977  0.010534  0.233220  0.549039  0.002594  0.511695  0.005134  0.003516   \n",
      "14978  0.010558  0.233425  0.549829  0.002607  0.511930  0.005137  0.003529   \n",
      "14979  0.010568  0.234561  0.551322  0.002605  0.511695  0.005137  0.003544   \n",
      "\n",
      "             O2        P8        T8       FC6        F4        F8       AF4  \\\n",
      "0      0.027383  0.010837  0.498575  0.264230  0.426286  0.029886  0.004237   \n",
      "1      0.026619  0.010795  0.496148  0.263219  0.426071  0.029865  0.004224   \n",
      "2      0.023390  0.010783  0.495197  0.262931  0.426610  0.029838  0.004231   \n",
      "3      0.023579  0.010820  0.497940  0.264086  0.427799  0.029862  0.004241   \n",
      "4      0.022437  0.010795  0.499735  0.264664  0.427909  0.029865  0.004244   \n",
      "5      0.018443  0.010763  0.497414  0.263796  0.426395  0.029835  0.004232   \n",
      "6      0.018065  0.010800  0.496148  0.261342  0.424016  0.029815  0.004216   \n",
      "7      0.017683  0.010775  0.496887  0.259897  0.423369  0.029795  0.004219   \n",
      "8      0.015214  0.010707  0.496780  0.261630  0.424882  0.029828  0.004232   \n",
      "9      0.015403  0.010732  0.496570  0.264664  0.425746  0.029896  0.004237   \n",
      "10     0.018065  0.010777  0.496570  0.264086  0.424665  0.029859  0.004222   \n",
      "11     0.017872  0.010738  0.495514  0.260331  0.422395  0.029737  0.004205   \n",
      "12     0.015974  0.010711  0.495092  0.258308  0.421099  0.029690  0.004204   \n",
      "13     0.018443  0.010775  0.495936  0.259463  0.422720  0.029717  0.004206   \n",
      "14     0.022819  0.010814  0.495831  0.260762  0.424990  0.029727  0.004212   \n",
      "15     0.022437  0.010777  0.495726  0.260909  0.423908  0.029717  0.004217   \n",
      "16     0.019774  0.010763  0.496041  0.259320  0.420774  0.029690  0.004204   \n",
      "17     0.018254  0.010767  0.496041  0.258018  0.420667  0.029690  0.004204   \n",
      "18     0.016163  0.010725  0.495409  0.258886  0.422937  0.029747  0.004221   \n",
      "19     0.014832  0.010709  0.494141  0.261052  0.424125  0.029791  0.004217   \n",
      "20     0.016923  0.010779  0.495302  0.263796  0.424558  0.029812  0.004220   \n",
      "21     0.018825  0.010804  0.498996  0.266109  0.424990  0.029862  0.004247   \n",
      "22     0.018825  0.010771  0.499948  0.265675  0.424990  0.029906  0.004248   \n",
      "23     0.020156  0.010804  0.498575  0.263509  0.424340  0.029876  0.004225   \n",
      "24     0.023390  0.010868  0.499418  0.263509  0.424125  0.029832  0.004217   \n",
      "25     0.026619  0.010874  0.500474  0.263652  0.424016  0.029815  0.004227   \n",
      "26     0.024910  0.010789  0.497309  0.260331  0.422395  0.029751  0.004219   \n",
      "27     0.018636  0.010713  0.494036  0.257151  0.420667  0.029677  0.004193   \n",
      "28     0.016163  0.010727  0.494036  0.258162  0.421314  0.029684  0.004190   \n",
      "29     0.018254  0.010754  0.494880  0.259897  0.423152  0.029724  0.004210   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14950  0.020156  0.010725  0.495726  0.259173  0.427692  0.029758  0.004221   \n",
      "14951  0.018443  0.010727  0.495302  0.258739  0.428556  0.029747  0.004215   \n",
      "14952  0.019774  0.010771  0.497309  0.261196  0.428663  0.029788  0.004224   \n",
      "14953  0.022630  0.010791  0.498257  0.262641  0.428663  0.029822  0.004231   \n",
      "14954  0.022248  0.010779  0.497201  0.260762  0.428880  0.029815  0.004224   \n",
      "14955  0.020916  0.010769  0.497309  0.260041  0.429312  0.029785  0.004214   \n",
      "14956  0.021298  0.010758  0.497519  0.260331  0.428663  0.029764  0.004212   \n",
      "14957  0.021298  0.010758  0.496148  0.259030  0.427367  0.029771  0.004214   \n",
      "14958  0.022819  0.010775  0.496358  0.258018  0.426610  0.029771  0.004210   \n",
      "14959  0.026812  0.010808  0.496992  0.257297  0.427152  0.029731  0.004201   \n",
      "14960  0.029474  0.010841  0.496675  0.255995  0.426718  0.029690  0.004196   \n",
      "14961  0.029092  0.010829  0.496675  0.255562  0.425529  0.029663  0.004194   \n",
      "14962  0.026241  0.010765  0.496148  0.255274  0.425854  0.029663  0.004190   \n",
      "14963  0.021866  0.010740  0.495619  0.254840  0.425961  0.029670  0.004190   \n",
      "14964  0.020727  0.010775  0.496148  0.254840  0.425639  0.029657  0.004193   \n",
      "14965  0.023579  0.010798  0.496253  0.254984  0.425854  0.029663  0.004189   \n",
      "14966  0.023957  0.010783  0.495831  0.254407  0.425746  0.029687  0.004183   \n",
      "14967  0.023008  0.010771  0.495831  0.253973  0.425529  0.029663  0.004179   \n",
      "14968  0.025099  0.010783  0.496358  0.254407  0.425746  0.029646  0.004177   \n",
      "14969  0.024528  0.010777  0.496675  0.255562  0.425961  0.029694  0.004178   \n",
      "14970  0.021487  0.010756  0.496148  0.255418  0.426071  0.029710  0.004182   \n",
      "14971  0.023390  0.010781  0.496041  0.254840  0.426395  0.029684  0.004178   \n",
      "14972  0.027383  0.010808  0.496570  0.254984  0.424882  0.029650  0.004166   \n",
      "14973  0.026812  0.010795  0.495936  0.254116  0.423584  0.029623  0.004162   \n",
      "14974  0.024150  0.010783  0.494775  0.253685  0.424340  0.029619  0.004166   \n",
      "14975  0.021677  0.010765  0.495092  0.252961  0.423908  0.029606  0.004163   \n",
      "14976  0.020156  0.010732  0.494248  0.250505  0.421856  0.029586  0.004153   \n",
      "14977  0.020916  0.010729  0.493297  0.249927  0.421531  0.029596  0.004161   \n",
      "14978  0.022819  0.010754  0.494775  0.251373  0.423476  0.029626  0.004177   \n",
      "14979  0.026430  0.010800  0.496148  0.251950  0.424990  0.029636  0.004177   \n",
      "\n",
      "       eyeDetection  \n",
      "0               0.0  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "5               0.0  \n",
      "6               0.0  \n",
      "7               0.0  \n",
      "8               0.0  \n",
      "9               0.0  \n",
      "10              0.0  \n",
      "11              0.0  \n",
      "12              0.0  \n",
      "13              0.0  \n",
      "14              0.0  \n",
      "15              0.0  \n",
      "16              0.0  \n",
      "17              0.0  \n",
      "18              0.0  \n",
      "19              0.0  \n",
      "20              0.0  \n",
      "21              0.0  \n",
      "22              0.0  \n",
      "23              0.0  \n",
      "24              0.0  \n",
      "25              0.0  \n",
      "26              0.0  \n",
      "27              0.0  \n",
      "28              0.0  \n",
      "29              0.0  \n",
      "...             ...  \n",
      "14950           0.0  \n",
      "14951           0.0  \n",
      "14952           0.0  \n",
      "14953           0.0  \n",
      "14954           0.0  \n",
      "14955           0.0  \n",
      "14956           0.0  \n",
      "14957           0.0  \n",
      "14958           0.0  \n",
      "14959           1.0  \n",
      "14960           1.0  \n",
      "14961           1.0  \n",
      "14962           1.0  \n",
      "14963           1.0  \n",
      "14964           1.0  \n",
      "14965           1.0  \n",
      "14966           1.0  \n",
      "14967           1.0  \n",
      "14968           1.0  \n",
      "14969           1.0  \n",
      "14970           1.0  \n",
      "14971           1.0  \n",
      "14972           1.0  \n",
      "14973           1.0  \n",
      "14974           1.0  \n",
      "14975           1.0  \n",
      "14976           1.0  \n",
      "14977           1.0  \n",
      "14978           1.0  \n",
      "14979           1.0  \n",
      "\n",
      "[14980 rows x 15 columns]>\n",
      "            AF3        F7        F3       FC5        T7        P7        O1  \\\n",
      "0      0.010702  0.236931  0.556326  0.002648  0.515557  0.005053  0.003558   \n",
      "1      0.010687  0.236004  0.557117  0.002649  0.513684  0.005054  0.003559   \n",
      "2      0.010697  0.236416  0.557379  0.002661  0.512514  0.005046  0.003558   \n",
      "3      0.010701  0.237446  0.557556  0.002660  0.514035  0.005043  0.003559   \n",
      "4      0.010692  0.237446  0.556854  0.002653  0.514971  0.005054  0.003556   \n",
      "5      0.010676  0.236004  0.555448  0.002656  0.514503  0.005056  0.003552   \n",
      "6      0.010671  0.235283  0.554833  0.002653  0.514035  0.005048  0.003546   \n",
      "7      0.010691  0.236416  0.554482  0.002640  0.514152  0.005044  0.003541   \n",
      "8      0.010692  0.237241  0.554131  0.002634  0.514387  0.005047  0.003548   \n",
      "9      0.010692  0.237343  0.554219  0.002638  0.514152  0.005043  0.003551   \n",
      "10     0.010692  0.237241  0.553517  0.002640  0.513801  0.005034  0.003542   \n",
      "11     0.010662  0.235590  0.551234  0.002629  0.513100  0.005040  0.003539   \n",
      "12     0.010634  0.233734  0.549214  0.002611  0.512282  0.005048  0.003546   \n",
      "13     0.010634  0.234766  0.549916  0.002608  0.512865  0.005048  0.003548   \n",
      "14     0.010649  0.236312  0.551409  0.002616  0.513452  0.005053  0.003546   \n",
      "15     0.010652  0.235797  0.550620  0.002616  0.512749  0.005053  0.003548   \n",
      "16     0.010629  0.234663  0.549127  0.002608  0.512514  0.005047  0.003554   \n",
      "17     0.010619  0.234354  0.549478  0.002607  0.513452  0.005047  0.003547   \n",
      "18     0.010647  0.235075  0.550620  0.002610  0.513684  0.005054  0.003538   \n",
      "19     0.010659  0.235797  0.550883  0.002608  0.513568  0.005058  0.003543   \n",
      "20     0.010654  0.236209  0.551234  0.002609  0.514387  0.005058  0.003550   \n",
      "21     0.010687  0.237653  0.551849  0.002623  0.513919  0.005056  0.003549   \n",
      "22     0.010709  0.239921  0.552375  0.002635  0.512398  0.005051  0.003546   \n",
      "23     0.010694  0.239096  0.552024  0.002631  0.511930  0.005053  0.003543   \n",
      "24     0.010677  0.236622  0.551322  0.002623  0.513217  0.005051  0.003541   \n",
      "25     0.010664  0.236622  0.550883  0.002617  0.514035  0.005050  0.003543   \n",
      "26     0.010646  0.236519  0.549741  0.002612  0.512398  0.005056  0.003542   \n",
      "27     0.010636  0.235075  0.548776  0.002606  0.510760  0.005051  0.003540   \n",
      "28     0.010634  0.235488  0.550092  0.002608  0.511579  0.005043  0.003542   \n",
      "29     0.010644  0.237343  0.552200  0.002620  0.513100  0.005046  0.003546   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14950  0.010706  0.242396  0.555097  0.002637  0.513452  0.005133  0.003507   \n",
      "14951  0.010691  0.242603  0.554570  0.002643  0.512282  0.005118  0.003505   \n",
      "14952  0.010694  0.242910  0.554833  0.002649  0.511930  0.005103  0.003505   \n",
      "14953  0.010697  0.242808  0.554833  0.002645  0.513217  0.005117  0.003514   \n",
      "14954  0.010701  0.242603  0.554659  0.002649  0.513684  0.005137  0.003518   \n",
      "14955  0.010699  0.242498  0.554922  0.002656  0.512282  0.005137  0.003520   \n",
      "14956  0.010679  0.242086  0.554746  0.002653  0.511463  0.005127  0.003523   \n",
      "14957  0.010659  0.241467  0.553955  0.002649  0.512633  0.005131  0.003524   \n",
      "14958  0.010659  0.240643  0.554044  0.002649  0.512982  0.005148  0.003525   \n",
      "14959  0.010647  0.239509  0.554482  0.002649  0.512514  0.005157  0.003531   \n",
      "14960  0.010621  0.238477  0.553693  0.002643  0.513217  0.005157  0.003536   \n",
      "14961  0.010617  0.238477  0.553166  0.002633  0.513684  0.005151  0.003533   \n",
      "14962  0.010622  0.238787  0.553429  0.002634  0.513217  0.005145  0.003533   \n",
      "14963  0.010611  0.237860  0.552815  0.002635  0.513452  0.005140  0.003532   \n",
      "14964  0.010607  0.236931  0.551936  0.002633  0.513801  0.005141  0.003535   \n",
      "14965  0.010616  0.236829  0.552200  0.002633  0.512398  0.005143  0.003544   \n",
      "14966  0.010601  0.236312  0.552375  0.002630  0.511112  0.005135  0.003543   \n",
      "14967  0.010588  0.236107  0.551760  0.002627  0.511579  0.005131  0.003535   \n",
      "14968  0.010604  0.236416  0.551498  0.002631  0.512749  0.005134  0.003541   \n",
      "14969  0.010597  0.236312  0.552024  0.002627  0.513100  0.005145  0.003547   \n",
      "14970  0.010573  0.235695  0.551936  0.002611  0.512865  0.005150  0.003536   \n",
      "14971  0.010571  0.234973  0.550356  0.002605  0.512514  0.005144  0.003533   \n",
      "14972  0.010569  0.234251  0.549303  0.002604  0.511930  0.005135  0.003537   \n",
      "14973  0.010551  0.233425  0.549654  0.002597  0.511463  0.005127  0.003524   \n",
      "14974  0.010544  0.232808  0.549478  0.002599  0.511463  0.005127  0.003515   \n",
      "14975  0.010546  0.233117  0.548863  0.002599  0.511814  0.005131  0.003519   \n",
      "14976  0.010533  0.233425  0.548776  0.002589  0.511579  0.005134  0.003517   \n",
      "14977  0.010534  0.233220  0.549039  0.002594  0.511695  0.005134  0.003516   \n",
      "14978  0.010558  0.233425  0.549829  0.002607  0.511930  0.005137  0.003529   \n",
      "14979  0.010568  0.234561  0.551322  0.002605  0.511695  0.005137  0.003544   \n",
      "\n",
      "             O2        P8        T8       FC6        F4        F8       AF4  \\\n",
      "0      0.027383  0.010837  0.498575  0.264230  0.426286  0.029886  0.004237   \n",
      "1      0.026619  0.010795  0.496148  0.263219  0.426071  0.029865  0.004224   \n",
      "2      0.023390  0.010783  0.495197  0.262931  0.426610  0.029838  0.004231   \n",
      "3      0.023579  0.010820  0.497940  0.264086  0.427799  0.029862  0.004241   \n",
      "4      0.022437  0.010795  0.499735  0.264664  0.427909  0.029865  0.004244   \n",
      "5      0.018443  0.010763  0.497414  0.263796  0.426395  0.029835  0.004232   \n",
      "6      0.018065  0.010800  0.496148  0.261342  0.424016  0.029815  0.004216   \n",
      "7      0.017683  0.010775  0.496887  0.259897  0.423369  0.029795  0.004219   \n",
      "8      0.015214  0.010707  0.496780  0.261630  0.424882  0.029828  0.004232   \n",
      "9      0.015403  0.010732  0.496570  0.264664  0.425746  0.029896  0.004237   \n",
      "10     0.018065  0.010777  0.496570  0.264086  0.424665  0.029859  0.004222   \n",
      "11     0.017872  0.010738  0.495514  0.260331  0.422395  0.029737  0.004205   \n",
      "12     0.015974  0.010711  0.495092  0.258308  0.421099  0.029690  0.004204   \n",
      "13     0.018443  0.010775  0.495936  0.259463  0.422720  0.029717  0.004206   \n",
      "14     0.022819  0.010814  0.495831  0.260762  0.424990  0.029727  0.004212   \n",
      "15     0.022437  0.010777  0.495726  0.260909  0.423908  0.029717  0.004217   \n",
      "16     0.019774  0.010763  0.496041  0.259320  0.420774  0.029690  0.004204   \n",
      "17     0.018254  0.010767  0.496041  0.258018  0.420667  0.029690  0.004204   \n",
      "18     0.016163  0.010725  0.495409  0.258886  0.422937  0.029747  0.004221   \n",
      "19     0.014832  0.010709  0.494141  0.261052  0.424125  0.029791  0.004217   \n",
      "20     0.016923  0.010779  0.495302  0.263796  0.424558  0.029812  0.004220   \n",
      "21     0.018825  0.010804  0.498996  0.266109  0.424990  0.029862  0.004247   \n",
      "22     0.018825  0.010771  0.499948  0.265675  0.424990  0.029906  0.004248   \n",
      "23     0.020156  0.010804  0.498575  0.263509  0.424340  0.029876  0.004225   \n",
      "24     0.023390  0.010868  0.499418  0.263509  0.424125  0.029832  0.004217   \n",
      "25     0.026619  0.010874  0.500474  0.263652  0.424016  0.029815  0.004227   \n",
      "26     0.024910  0.010789  0.497309  0.260331  0.422395  0.029751  0.004219   \n",
      "27     0.018636  0.010713  0.494036  0.257151  0.420667  0.029677  0.004193   \n",
      "28     0.016163  0.010727  0.494036  0.258162  0.421314  0.029684  0.004190   \n",
      "29     0.018254  0.010754  0.494880  0.259897  0.423152  0.029724  0.004210   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14950  0.020156  0.010725  0.495726  0.259173  0.427692  0.029758  0.004221   \n",
      "14951  0.018443  0.010727  0.495302  0.258739  0.428556  0.029747  0.004215   \n",
      "14952  0.019774  0.010771  0.497309  0.261196  0.428663  0.029788  0.004224   \n",
      "14953  0.022630  0.010791  0.498257  0.262641  0.428663  0.029822  0.004231   \n",
      "14954  0.022248  0.010779  0.497201  0.260762  0.428880  0.029815  0.004224   \n",
      "14955  0.020916  0.010769  0.497309  0.260041  0.429312  0.029785  0.004214   \n",
      "14956  0.021298  0.010758  0.497519  0.260331  0.428663  0.029764  0.004212   \n",
      "14957  0.021298  0.010758  0.496148  0.259030  0.427367  0.029771  0.004214   \n",
      "14958  0.022819  0.010775  0.496358  0.258018  0.426610  0.029771  0.004210   \n",
      "14959  0.026812  0.010808  0.496992  0.257297  0.427152  0.029731  0.004201   \n",
      "14960  0.029474  0.010841  0.496675  0.255995  0.426718  0.029690  0.004196   \n",
      "14961  0.029092  0.010829  0.496675  0.255562  0.425529  0.029663  0.004194   \n",
      "14962  0.026241  0.010765  0.496148  0.255274  0.425854  0.029663  0.004190   \n",
      "14963  0.021866  0.010740  0.495619  0.254840  0.425961  0.029670  0.004190   \n",
      "14964  0.020727  0.010775  0.496148  0.254840  0.425639  0.029657  0.004193   \n",
      "14965  0.023579  0.010798  0.496253  0.254984  0.425854  0.029663  0.004189   \n",
      "14966  0.023957  0.010783  0.495831  0.254407  0.425746  0.029687  0.004183   \n",
      "14967  0.023008  0.010771  0.495831  0.253973  0.425529  0.029663  0.004179   \n",
      "14968  0.025099  0.010783  0.496358  0.254407  0.425746  0.029646  0.004177   \n",
      "14969  0.024528  0.010777  0.496675  0.255562  0.425961  0.029694  0.004178   \n",
      "14970  0.021487  0.010756  0.496148  0.255418  0.426071  0.029710  0.004182   \n",
      "14971  0.023390  0.010781  0.496041  0.254840  0.426395  0.029684  0.004178   \n",
      "14972  0.027383  0.010808  0.496570  0.254984  0.424882  0.029650  0.004166   \n",
      "14973  0.026812  0.010795  0.495936  0.254116  0.423584  0.029623  0.004162   \n",
      "14974  0.024150  0.010783  0.494775  0.253685  0.424340  0.029619  0.004166   \n",
      "14975  0.021677  0.010765  0.495092  0.252961  0.423908  0.029606  0.004163   \n",
      "14976  0.020156  0.010732  0.494248  0.250505  0.421856  0.029586  0.004153   \n",
      "14977  0.020916  0.010729  0.493297  0.249927  0.421531  0.029596  0.004161   \n",
      "14978  0.022819  0.010754  0.494775  0.251373  0.423476  0.029626  0.004177   \n",
      "14979  0.026430  0.010800  0.496148  0.251950  0.424990  0.029636  0.004177   \n",
      "\n",
      "       eyeDetection  \n",
      "0               0.0  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "5               0.0  \n",
      "6               0.0  \n",
      "7               0.0  \n",
      "8               0.0  \n",
      "9               0.0  \n",
      "10              0.0  \n",
      "11              0.0  \n",
      "12              0.0  \n",
      "13              0.0  \n",
      "14              0.0  \n",
      "15              0.0  \n",
      "16              0.0  \n",
      "17              0.0  \n",
      "18              0.0  \n",
      "19              0.0  \n",
      "20              0.0  \n",
      "21              0.0  \n",
      "22              0.0  \n",
      "23              0.0  \n",
      "24              0.0  \n",
      "25              0.0  \n",
      "26              0.0  \n",
      "27              0.0  \n",
      "28              0.0  \n",
      "29              0.0  \n",
      "...             ...  \n",
      "14950           0.0  \n",
      "14951           0.0  \n",
      "14952           0.0  \n",
      "14953           0.0  \n",
      "14954           0.0  \n",
      "14955           0.0  \n",
      "14956           0.0  \n",
      "14957           0.0  \n",
      "14958           0.0  \n",
      "14959           1.0  \n",
      "14960           1.0  \n",
      "14961           1.0  \n",
      "14962           1.0  \n",
      "14963           1.0  \n",
      "14964           1.0  \n",
      "14965           1.0  \n",
      "14966           1.0  \n",
      "14967           1.0  \n",
      "14968           1.0  \n",
      "14969           1.0  \n",
      "14970           1.0  \n",
      "14971           1.0  \n",
      "14972           1.0  \n",
      "14973           1.0  \n",
      "14974           1.0  \n",
      "14975           1.0  \n",
      "14976           1.0  \n",
      "14977           1.0  \n",
      "14978           1.0  \n",
      "14979           1.0  \n",
      "\n",
      "[14980 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "data = arff.loadarff('EEG_Eye_State.arff')\n",
    "dataset = pd.DataFrame(data[0]).astype('float64')\n",
    "\n",
    "#Data pre-processing\n",
    "min_max = MinMaxScaler()\n",
    "dataset.iloc[:,0:14]= min_max.fit_transform(dataset.iloc[:,0:14] )\n",
    "\n",
    "print(dataset.head)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(data, n_prev = 10):  \n",
    "\n",
    "    docX, docY = [], []\n",
    "    #docX = []\n",
    "    for i in range(len(data)-n_prev+1):\n",
    "        docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
    "        docY.append(data.iloc[14].as_matrix())\n",
    "    X_new = np.array(docX)\n",
    "    Y_new = np.array(docY)\n",
    "\n",
    "    return X_new, Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahesh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/home/mahesh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14971, 10, 14)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data =_load_data(dataset,10)\n",
    "\n",
    "# Y_data =_load_data(Y,1)\n",
    "Y_data = Y_data[:,14]\n",
    "X_data = X_data[:, :, :14]\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.2, random_state = 46)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.0033, random_state = 46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_85 (LSTM)               (None, 10, 20)            2800      \n",
      "_________________________________________________________________\n",
      "lstm_86 (LSTM)               (None, 10)                1240      \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,051\n",
      "Trainable params: 4,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#accuracy is not viewed since it can be misleading\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(20,return_sequences=True, batch_input_shape=(None, 10, 14)))\n",
    "model.add(LSTM(10,return_sequences=False, batch_input_shape=(None, 10, 14)))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(optimizer = 'adam', loss='binary_crossentropy')\n",
    "\n",
    "model.summary()\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11976 samples, validate on 2985 samples\n",
      "Epoch 1/500\n",
      "11976/11976 [==============================] - 29s 2ms/step - loss: 0.0711 - val_loss: 0.0066\n",
      "Epoch 2/500\n",
      "11976/11976 [==============================] - 16s 1ms/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 3/500\n",
      "11976/11976 [==============================] - 20s 2ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 4/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 5/500\n",
      "11976/11976 [==============================] - 20s 2ms/step - loss: 8.8094e-04 - val_loss: 7.3426e-04\n",
      "Epoch 6/500\n",
      "11976/11976 [==============================] - 21s 2ms/step - loss: 6.2372e-04 - val_loss: 5.2547e-04\n",
      "Epoch 7/500\n",
      "11976/11976 [==============================] - 23s 2ms/step - loss: 4.6218e-04 - val_loss: 4.0482e-04\n",
      "Epoch 8/500\n",
      "11976/11976 [==============================] - 24s 2ms/step - loss: 3.5866e-04 - val_loss: 3.1634e-04\n",
      "Epoch 9/500\n",
      "11976/11976 [==============================] - 36s 3ms/step - loss: 2.8181e-04 - val_loss: 2.4995e-04\n",
      "Epoch 10/500\n",
      "11976/11976 [==============================] - 36s 3ms/step - loss: 2.2364e-04 - val_loss: 1.9924e-04\n",
      "Epoch 11/500\n",
      "11976/11976 [==============================] - 42s 4ms/step - loss: 1.7891e-04 - val_loss: 1.5997e-04\n",
      "Epoch 12/500\n",
      "11976/11976 [==============================] - 25s 2ms/step - loss: 1.4407e-04 - val_loss: 1.2919e-04\n",
      "Epoch 13/500\n",
      "11976/11976 [==============================] - 29s 2ms/step - loss: 1.1662e-04 - val_loss: 1.0483e-04\n",
      "Epoch 14/500\n",
      "11976/11976 [==============================] - 27s 2ms/step - loss: 9.4815e-05 - val_loss: 8.5396e-05\n",
      "Epoch 15/500\n",
      "11976/11976 [==============================] - 20s 2ms/step - loss: 7.7358e-05 - val_loss: 6.9784e-05\n",
      "Epoch 16/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 6.3298e-05 - val_loss: 5.7176e-05\n",
      "Epoch 17/500\n",
      "11976/11976 [==============================] - 30s 3ms/step - loss: 5.1916e-05 - val_loss: 4.6946e-05\n",
      "Epoch 18/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 4.2665e-05 - val_loss: 3.8614e-05\n",
      "Epoch 19/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 3.5118e-05 - val_loss: 3.1807e-05\n",
      "Epoch 20/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 2.8945e-05 - val_loss: 2.6232e-05\n",
      "Epoch 21/500\n",
      "11976/11976 [==============================] - 16s 1ms/step - loss: 2.3884e-05 - val_loss: 2.1656e-05\n",
      "Epoch 22/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 1.9725e-05 - val_loss: 1.7892e-05\n",
      "Epoch 23/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 1.6302e-05 - val_loss: 1.4793e-05\n",
      "Epoch 24/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 1.3482e-05 - val_loss: 1.2237e-05\n",
      "Epoch 25/500\n",
      "11976/11976 [==============================] - 21s 2ms/step - loss: 1.1155e-05 - val_loss: 1.0128e-05\n",
      "Epoch 26/500\n",
      "11976/11976 [==============================] - 23s 2ms/step - loss: 9.2343e-06 - val_loss: 8.3851e-06\n",
      "Epoch 27/500\n",
      "11976/11976 [==============================] - 18s 1ms/step - loss: 7.6467e-06 - val_loss: 6.9447e-06\n",
      "Epoch 28/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 6.3340e-06 - val_loss: 5.7532e-06\n",
      "Epoch 29/500\n",
      "11976/11976 [==============================] - 18s 1ms/step - loss: 5.2478e-06 - val_loss: 4.7671e-06\n",
      "Epoch 30/500\n",
      "11976/11976 [==============================] - 16s 1ms/step - loss: 4.3488e-06 - val_loss: 3.9509e-06\n",
      "Epoch 31/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 3.6046e-06 - val_loss: 3.2751e-06\n",
      "Epoch 32/500\n",
      "11976/11976 [==============================] - 16s 1ms/step - loss: 2.9882e-06 - val_loss: 2.7152e-06\n",
      "Epoch 33/500\n",
      "11976/11976 [==============================] - 16s 1ms/step - loss: 2.4775e-06 - val_loss: 2.2514e-06\n",
      "Epoch 34/500\n",
      "11976/11976 [==============================] - 16s 1ms/step - loss: 2.0544e-06 - val_loss: 1.8670e-06\n",
      "Epoch 35/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.7038e-06 - val_loss: 1.5485e-06\n",
      "Epoch 36/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.4132e-06 - val_loss: 1.2844e-06\n",
      "Epoch 37/500\n",
      "11976/11976 [==============================] - 14s 1ms/step - loss: 1.1722e-06 - val_loss: 1.0655e-06\n",
      "Epoch 38/500\n",
      "11976/11976 [==============================] - 13s 1ms/step - loss: 9.7247e-07 - val_loss: 8.8392e-07\n",
      "Epoch 39/500\n",
      "11976/11976 [==============================] - 14s 1ms/step - loss: 8.0681e-07 - val_loss: 7.3342e-07\n",
      "Epoch 40/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 6.6950e-07 - val_loss: 6.0865e-07\n",
      "Epoch 41/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 5.5566e-07 - val_loss: 5.0520e-07\n",
      "Epoch 42/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 4.6125e-07 - val_loss: 4.1941e-07\n",
      "Epoch 43/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 3.8297e-07 - val_loss: 3.4828e-07\n",
      "Epoch 44/500\n",
      "11976/11976 [==============================] - 12s 1ms/step - loss: 3.1806e-07 - val_loss: 2.8927e-07\n",
      "Epoch 45/500\n",
      "11976/11976 [==============================] - 11s 937us/step - loss: 2.6421e-07 - val_loss: 2.4034e-07\n",
      "Epoch 46/500\n",
      "11976/11976 [==============================] - 11s 910us/step - loss: 2.1956e-07 - val_loss: 1.9976e-07\n",
      "Epoch 47/500\n",
      "11976/11976 [==============================] - 11s 905us/step - loss: 1.8252e-07 - val_loss: 1.6611e-07\n",
      "Epoch 48/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.5181e-07 - val_loss: 1.3820e-07\n",
      "Epoch 49/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 1.2634e-07 - val_loss: 1.1504e-07\n",
      "Epoch 50/500\n",
      "11976/11976 [==============================] - 14s 1ms/step - loss: 1.0568e-07 - val_loss: 1.0000e-07\n",
      "Epoch 51/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 52/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 53/500\n",
      "11976/11976 [==============================] - 14s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 54/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 55/500\n",
      "11976/11976 [==============================] - 14s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 56/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 57/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 58/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 59/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 60/500\n",
      "11976/11976 [==============================] - 18s 2ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 61/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 62/500\n",
      "11976/11976 [==============================] - 13s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 63/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 64/500\n",
      "11976/11976 [==============================] - 18s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 65/500\n",
      "11976/11976 [==============================] - 17s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 66/500\n",
      "11976/11976 [==============================] - 19s 2ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 67/500\n",
      "11976/11976 [==============================] - 18s 1ms/step - loss: 1.0002e-07 - val_loss: 1.0000e-07\n",
      "Epoch 68/500\n",
      "11976/11976 [==============================] - 15s 1ms/step - loss: 1.0001e-07 - val_loss: 1.0000e-07\n",
      "Epoch 69/500\n",
      "11976/11976 [==============================] - 12s 981us/step - loss: 1.0001e-07 - val_loss: 1.0000e-07\n",
      "Epoch 70/500\n",
      "11976/11976 [==============================] - 11s 878us/step - loss: 1.0001e-07 - val_loss: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None)\n",
    "model_history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=500,  callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.legend(['Training'])\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "plot_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.predict_classes(x_val)\n",
    "\n",
    "plt.title(\"TESTING THE MODEL\")\n",
    "\n",
    "plt.scatter(range(10), results[:10,0], s = 100, c='r')\n",
    "plt.scatter(range(10), y_val[:10], s = 50, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 10 to array axis with dimension 14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7bce32bd527d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 10 to array axis with dimension 14"
     ]
    }
   ],
   "source": [
    "data = arff.loadarff('EEG_Eye_State.arff')\n",
    "dataset = pd.DataFrame(data[0]).astype('float64')\n",
    "\n",
    "#Data pre-processing\n",
    "min_max = MinMaxScaler()\n",
    "dataset.iloc[:,0:14]= min_max.fit_transform(dataset.iloc[:,0:14] )\n",
    "\n",
    "X_data = dataset.iloc[:, :14]\n",
    "Y_data = dataset.iloc[:, 14].values\n",
    "\n",
    "Y_data.reshape(1498, 10)\n",
    "\n",
    "samples = list() \n",
    "length = 10\n",
    "n = X_data.shape[0]\n",
    "\n",
    "for i in range(0,n,length):\n",
    "    sample = X_data.iloc[i:i+length]\n",
    "    samples.append(sample)\n",
    "\n",
    "X_data = array(samples)\n",
    "\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.2, random_state = 46)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.0033, random_state = 46)\n",
    "\n",
    "#Building the model and compiling\n",
    "model = Sequential()\n",
    "model.add(LSTM(20,return_sequences=True, batch_input_shape=(None, 10, 14)))\n",
    "model.add(LSTM(10,return_sequences=True))\n",
    "model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None)\n",
    "model_history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=500,  callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_dev",
   "language": "python",
   "name": "tensorflow_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
